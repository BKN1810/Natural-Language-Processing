{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e1ce694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\balkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eeee7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pleasant', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "text=nltk.word_tokenize(\"It is a pleasant day today.\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b3aa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('refuse', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('permit', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('obtain', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('refuse', 'NN'),\n",
       " ('permit', 'NN')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4861ebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "# NLTK may provide the information of tags.\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53070238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# NLTK may provide the information of tags.\n",
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2085c98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('VB.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79deb352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('bear', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('pain', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('bear', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=nltk.word_tokenize(\"I cannot bear the pain of bear.\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d82ba",
   "metadata": {},
   "source": [
    "## Tagged Corpora\n",
    "### Representing Tagged Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1072d1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bear', 'NN')\n",
      "bear\n",
      "NN\n"
     ]
    }
   ],
   "source": [
    "# In NLTK, a tagged token is represented as a tuple consisting of a token and its tag.\n",
    "# We can create this tuple in NLTK using the str2tuple() function\n",
    "\n",
    "taggedword=nltk.tag.str2tuple('bear/NN')\n",
    "print(taggedword)\n",
    "print(taggedword[0])\n",
    "print(taggedword[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5556c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People/NNP'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converts the tuple (word and pos tag) into a word and a tag\n",
    "from nltk.tag.util import tuple2str\n",
    "taggedtok = ('People', 'NNP')\n",
    "tuple2str(taggedtok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f8036cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='''The/DT sacred/VBN Ganga/NNP flows/VBZ in/IN this/DT\n",
    "region/NN ./. This/DT is/VBZ a/DT pilgrimage/NN ./. People/NNP from/IN\n",
    "all/DT over/IN the/DT country/NN visit/NN this/DT place/NN ./. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "736cec8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('sacred', 'VBN'),\n",
       " ('Ganga', 'NNP'),\n",
       " ('flows', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('region', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('pilgrimage', 'NN'),\n",
       " ('.', '.'),\n",
       " ('People', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('country', 'NN'),\n",
       " ('visit', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('place', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nltk.tag.str2tuple(t) for t in sentence.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fa29d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Tokenization', 'NN'), ('first', 'RB'), ('step', 'VB'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.')]\n",
      "[('The', 'DT'), ('process', 'NN'), ('breaking', 'VBG'), ('text', 'NN'), ('paragraph', 'NN'), ('smaller', 'JJR'), ('chunks', 'NNS'), ('words', 'NNS'), ('sentence', 'NN'), ('called', 'VBN'), ('Tokenization', 'NNP'), ('.', '.')]\n",
      "[('Token', 'NNP'), ('single', 'JJ'), ('entity', 'NN'), ('building', 'NN'), ('blocks', 'NNS'), ('sentence', 'NN'), ('paragraph', 'NN'), ('.', '.')]\n",
      "[('Does', 'NNP'), ('sentence', 'VB'), ('tokenizer', 'NN'), ('break', 'NN'), ('text', 'NN'), ('paragraph', 'NN'), ('sentences', 'NNS'), ('?', '.')]\n",
      "[('What', 'WP'), ('fact', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "text=\"\"\"Tokenization is the first step in text analytics. \n",
    "        The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization.\n",
    "        Token is a single entity that is building blocks for sentence or paragraph. \n",
    "        Does sentence tokenizer break text paragraph into sentences?\n",
    "        What is fact?\"\"\"\n",
    "\n",
    "\n",
    "tokenized = sent_tokenize(text)\n",
    "for i in tokenized:\n",
    "      \n",
    "    # Word tokenizers is used to find the words and punctuation in a string\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    "  \n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop_words] \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech agger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    "  \n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c4366f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\balkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\indian.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edc0fb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('মহিষের', 'NN'), ('সন্তান', 'NN'), (':', 'SYM'), ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.indian.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af96ece2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb031bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\balkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c586dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6c16950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 30654),\n",
       " ('VERB', 14399),\n",
       " ('ADP', 12355),\n",
       " ('.', 11928),\n",
       " ('DET', 11389),\n",
       " ('ADJ', 6706),\n",
       " ('ADV', 3349),\n",
       " ('CONJ', 2717),\n",
       " ('PRON', 2535),\n",
       " ('PRT', 2264),\n",
       " ('NUM', 2166),\n",
       " ('X', 92)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "tag_fd = nltk.FreqDist(tag for (word, tag) in brown_news_tagged)\n",
    "tag_fd.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11919735",
   "metadata": {},
   "source": [
    "## Default tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47ac0c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Default tagging provides a baseline for part-of-speech tagging. It simply assigns the same\\npart-of-speech tag to every token. We do this using the DefaultTagger class. This tagger\\nis useful as a last-resort tagger, and provides a baseline to measure accuracy improvements.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Default tagging provides a baseline for part-of-speech tagging. It simply assigns the same\n",
    "part-of-speech tag to every token. We do this using the DefaultTagger class. This tagger\n",
    "is useful as a last-resort tagger, and provides a baseline to measure accuracy improvements.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dba052d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'VB'), ('World', 'VB'), ('Students', 'VB')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger\n",
    "tagger = DefaultTagger('VB')\n",
    "tagger.tag(['Hello', 'World','Students'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "feb8da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Default', 'NN'), ('tagging', 'NN'), ('provides', 'NN'), ('a', 'NN'), ('baseline', 'NN'), ('for', 'NN'), ('part-of-speech', 'NN'), ('tagging', 'NN'), ('.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text=\"Default tagging provides a baseline for part-of-speech tagging.\"\n",
    "text=[nltk.word_tokenize(text)]\n",
    "tagger = DefaultTagger('NN')\n",
    "for tg in text:\n",
    "    print(tagger.tag(tg))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcbd9572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14331966328512843"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6ada7",
   "metadata": {},
   "source": [
    "## Training a unigram part-of-speech tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dd81ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A unigram generally refers to a single token. Therefore, a unigram tagger only uses a single\\nword as its context for determining the part-of-speech tag.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"A unigram generally refers to a single token. Therefore, a unigram tagger only uses a single\n",
    "word as its context for determining the part-of-speech tag.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "805bebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\balkr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6180b6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use the first 3000 tagged sentences of the treebank corpus as the training set to\\ninitialize the UnigramTagger class. Then, we see the first sentence as a list of words,\\nand can see how it is transformed by the tag() function into a list of tagged tokens'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We use the first 3000 tagged sentences of the treebank corpus as the training set to\n",
    "initialize the UnigramTagger class. Then, we see the first sentence as a list of words,\n",
    "and can see how it is transformed by the tag() function into a list of tagged tokens\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3847bf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pierre',\n",
       " 'Vinken',\n",
       " ',',\n",
       " '61',\n",
       " 'years',\n",
       " 'old',\n",
       " ',',\n",
       " 'will',\n",
       " 'join',\n",
       " 'the',\n",
       " 'board',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nonexecutive',\n",
       " 'director',\n",
       " 'Nov.',\n",
       " '29',\n",
       " '.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.corpus import treebank\n",
    "train_sents = treebank.tagged_sents()[:3000]\n",
    "tagger = UnigramTagger(train_sents)\n",
    "treebank.sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2bd13a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pierre', 'NNP'),\n",
       " ('Vinken', 'NNP'),\n",
       " (',', ','),\n",
       " ('61', 'CD'),\n",
       " ('years', 'NNS'),\n",
       " ('old', 'JJ'),\n",
       " (',', ','),\n",
       " ('will', 'MD'),\n",
       " ('join', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('board', 'NN'),\n",
       " ('as', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('nonexecutive', 'JJ'),\n",
       " ('director', 'NN'),\n",
       " ('Nov.', 'NNP'),\n",
       " ('29', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(treebank.sents()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a50ff9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571551910209367"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents = treebank.tagged_sents()[3000:]\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3ac0b6",
   "metadata": {},
   "source": [
    "## Minimum frequency cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "279f4bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.775350744657889"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"If you'd like to set a minimum frequency threshold, then you can pass a cutoff value to the\n",
    "UnigramTagger class.\"\"\"\n",
    "\n",
    "tagger = UnigramTagger(train_sents, cutoff=3)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4feb63bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7951651197927908"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = UnigramTagger(train_sents, cutoff=2)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87861bb2",
   "metadata": {},
   "source": [
    "## Combining taggers with backoff tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f484d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Backoff tagging is one of the core features of SequentialBackoffTagger. It allows you\n",
    "to chain taggers together so that if one tagger doesn't know how to tag a word, it can pass\n",
    "the word on to the next backoff tagger. If that one can't do it, it can pass the word on to the\n",
    "next backoff tagger, and so on until\n",
    "there are no backoff taggers left to check.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57941c48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8741204403194475"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger1 = DefaultTagger('NN')\n",
    "tagger2 = UnigramTagger(train_sents, backoff=tagger1)\n",
    "tagger2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68626a15",
   "metadata": {},
   "source": [
    "### Training and combining ngram taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64b09896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11318799913662854"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "bitagger = BigramTagger(train_sents)\n",
    "bitagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82f1e8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06902654867256637"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tritagger = TrigramTagger(train_sents)\n",
    "tritagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8803e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "870fe9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8806388948845241"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backoff = DefaultTagger('NN')\n",
    "tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger,TrigramTagger], backoff=backoff)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6268956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'taking', 'nlp', 'classes', '.']\n",
      "[('We', 'PRP'), ('are', 'VBP'), ('taking', 'VBG'), ('nlp', 'JJ'), ('classes', 'NNS'), ('.', '.')]\n",
      "(S We/PRP are/VBP taking/VBG nlp/JJ classes/NNS ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"We are taking nlp classes.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)\n",
    "result.draw()    # It will draw the pattern graphically which can be seen in Noun Phrase chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84a03c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
